import triton
import triton.language as tl
import torch


@triton.jit
def _static_per_tensor_fp8_quant_kernel(qx_ptr: torch.Tensor, 
                                        x_in_ptr: torch.Tensor, 
                                        scale_in_ptr: torch.Tensor,
                                        cols: int,
                                        x_in_stride_r: int,
                                        NUM_COL_POW2: tl.constexpr
):
    """
    #TODO: Add Doc
    """

    pid = tl.program_id(axis=0)
    tl.assume(pid > 0)
    tl.assume(x_in_stride_r > 0)

    offs = pid * x_in_stride_r + tl.arange(0, NUM_COL_POW2)
    mask = tl.arange(0, NUM_COL_POW2) < cols
    x = tl.load(x_in_ptr + offs, mask = mask, cache_modifier=".cg")

    scale = tl.load(scale_in_ptr)
    scale_recip = 1 / scale

    qx = (x * scale_recip).to(qx_ptr.dtype.element_ty)

    tl.store(qx_ptr + offs, qx, mask=mask)


def static_per_tensor_fp8_quant(qx: torch.Tensor, 
                                x_in: torch.Tensor, 
                                scale_in: torch.Tensor
):

    """
    #TODO: Add Doc
    """
    assert scale_in.numel() == 1 #only single scale value 
    rows = x_in.shape[0]
    cols = x_in.shape[1]
    NUM_COL_POW2 = triton.next_power_of_2(cols)
    grid = lambda meta: (rows,)
    _static_per_tensor_fp8_quant_kernel[grid](qx, 
                                            x_in, 
                                            scale_in, 
                                            cols, 
                                            x_in.stride(0),
                                            NUM_COL_POW2=NUM_COL_POW2
                                            )

    return qx


@triton.jit
def _dynamic_per_tensor_fp8_quant_kernel(x_in_ptr: torch.Tensor,
                                        scale_out_ptr: torch.Tensor,
                                        cols: int,
                                        x_in_stride_r: int,
                                        NUM_COL_POW2: tl.constexpr,
                                        FP8_MAX: tl.constexpr
):
    """
    #TODO: Add Doc
    """

    pid = tl.program_id(axis=0)
    tl.assume(pid > 0)
    tl.assume(x_in_stride_r > 0)

    offs = pid * x_in_stride_r + tl.arange(0, NUM_COL_POW2)
    mask = tl.arange(0, NUM_COL_POW2) < cols
    x = tl.load(x_in_ptr + offs, mask = mask, cache_modifier=".cg")

    m = tl.max(tl.abs(x))
    tl.atomic_max(scale_out_ptr, m / FP8_MAX, sem="relaxed")
    

def dynamic_per_tensor_fp8_quant(qx: torch.Tensor, 
                                x_in: torch.Tensor,
                                scale_out: torch.Tensor
):

    """
    #TODO: Add Doc
    """
    rows = x_in.shape[0]
    cols = x_in.shape[1]
    NUM_COL_POW2 = triton.next_power_of_2(cols)
    grid = lambda meta: (rows,)
    _dynamic_per_tensor_fp8_quant_kernel[grid](x_in,
                                            scale_out,
                                            cols,
                                            x_in.stride(0),
                                            NUM_COL_POW2=NUM_COL_POW2,
                                            FP8_MAX=torch.finfo(qx.dtype).max
                                            )

    _static_per_tensor_fp8_quant_kernel[grid](qx,
                                            x_in,
                                            scale_out,
                                            cols,
                                            x_in.stride(0),
                                            NUM_COL_POW2=NUM_COL_POW2)

    return qx, scale_out


@triton.jit
def _dynamic_per_token_fp8_quant_kernel(qx_ptr: torch.Tensor,
                                        scale_out_ptr: torch.Tensor,
                                        x_in_ptr: torch.Tensor,
                                        cols: int,
                                        x_in_stride_r: int,
                                        NUM_COL_POW2: tl.constexpr,
                                        FP8_MAX: tl.constexpr,
):
    """
    #TODO: Add Doc
    """

    pid = tl.program_id(axis=0)
    tl.assume(pid > 0)
    tl.assume(x_in_stride_r > 0)

    offs = pid * x_in_stride_r + tl.arange(0, NUM_COL_POW2)
    mask = tl.arange(0, NUM_COL_POW2) < cols
    x = tl.load(x_in_ptr + offs, mask=mask, cache_modifier=".cg")

    m = tl.max(tl.abs(x), axis=-1)
    scale_out = m / FP8_MAX
    scale_recip = 1 / scale_out

    qx = x * scale_recip
    qx = qx.to(qx_ptr.dtype.element_ty)

    scale_offs = pid
    tl.store(scale_out_ptr + scale_offs, scale_out)

    tl.store(qx_ptr + offs, qx, mask=mask, cache_modifier=".cs")


def dynamic_per_token_fp8_quant(qx: torch.Tensor,
                                x_in: torch.Tensor,
                                scale_out: torch.Tensor,
                                quant_dtype=torch.float8_e4m3fnuz,
                                dtypeMax:torch.Tensor=torch.finfo(torch.float8_e4m3fnuz).max
):

    """
    #TODO: Add doc
    """
    rows = x_in.shape[0]
    cols = x_in.shape[1]
    NUM_COL_POW2 = triton.next_power_of_2(cols)
    grid = lambda meta: (rows,)
    _dynamic_per_token_fp8_quant_kernel[grid](qx, 
                                            scale_out,
                                            x_in, 
                                            cols, 
                                            x_in.stride(0),
                                            NUM_COL_POW2=NUM_COL_POW2,
                                            FP8_MAX=dtypeMax,
                                            )

    return qx, scale_out


@triton.jit
def _dynamic_mxfp4_quant_kernel(x_ptr,
                                x_fp4_ptr,
                                bs_ptr,
                                stride_x_m, stride_x_n,
                                stride_x_fp4_m, stride_x_fp4_n,
                                stride_bs_m, stride_bs_n,
                                M: tl.constexpr,
                                N: tl.constexpr,
                                BLOCK_SIZE: tl.constexpr,
                                MXFP4_QUANT_BLOCK_SIZE: tl.constexpr,
                                SCALING_MODE: tl.constexpr
                        ):
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    x_offs_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE) 
    x_offs_n = pid_n * MXFP4_QUANT_BLOCK_SIZE + tl.arange(0, MXFP4_QUANT_BLOCK_SIZE)
    x_offs = x_offs_m[:, None] * stride_x_m + x_offs_n[None, :] * stride_x_n
    x_mask = (x_offs_m < M)[:,None] & (x_offs_n < N)[None, :]
    x = tl.load(x_ptr + x_offs, mask=x_mask).to(tl.float32)

    #Calculate scale
    amax = tl.max(tl.abs(x), axis=1, keep_dims=True) 
    amax = tl.floor(amax + 0.5) #Rounding to nearest
    dequant_scale = amax / 6.0
    dequant_scale_rounded = dequant_scale.to(tl.float32, bitcast=True) 

    # Reciprocal of scale ie quant scale
    quant_scale = tl.where(dequant_scale_rounded == 0, 0, 1.0 / dequant_scale_rounded)

    #Compute quantized x
    qx = x * quant_scale

    # blockscale_e8m0
    bs_e8m0 = dequant_scale.to(tl.uint32, bitcast=True) & 0x7F800000
    bs_e8m0 = (bs_e8m0 >> 23).to(tl.uint8)

    # Convert quantized fp32 tensor to uint32 before converting to mxfp4 format
    # Note: MXFP4  S:1-bit, E:2-bit, M:1-bit
    #   Zeros: S000 -> +/-0
    #   Denormal Numbers: S001 -> +/- 0.5
    #   Normal Numbers: 
    #           S010 -> +/- 1.0
    #           S011 -> +/- 1.5
    #           S100 -> +/- 2.0
    #           S101 -> +/- 3.0
    #           S110 -> +/- 4.0
    #           S111 -> +/- 6.0
    qx = qx.to(tl.uint32, bitcast=True)

    #Extract sign, exponents and mantissa fields from FP32
    s = qx & 0x80000000
    e = (qx >> 23) & 0xFF
    m = (qx & 0x7FFFFF)

    E8_BIAS:tl.constexpr = 127
    E2_BIAS:tl.constexpr = 1

    #Denormal numbers
    #If exponent is less than 127, then it's a denormal number
    #See above, for denormal number mantissa is always 1 and we set bit 1 of mantissa
    adjusted_exponents = tl.core.sub(E8_BIAS, e + 1, sanitize_overflow=False)
    m = tl.where(e < E8_BIAS, (0x400000 | (m >> 1)) >> adjusted_exponents, m)

    # For normal numbers, bias is changed from 127 to 1, and for subnormals, we keep exponent as 0.
    #Note: E8_BIAS - E2_BIAS = 126, so for normals we subtract that.
    e = tl.maximum(e, E8_BIAS - E2_BIAS) - (E8_BIAS - E2_BIAS)

    # Combine sign, exponent, and mantissa, while saturating
    # rounding nearest with tie breaking up by adding +1 to one bit right of the LSB, then shift right
    e2m1_tmp = tl.minimum((((e << 2) | (m >> 21)) + 1) >> 1, 0x7)
    e2m1_value = ((s >> 28) | e2m1_tmp).to(tl.uint8)

    e2m1_value = tl.reshape(e2m1_value, [BLOCK_SIZE, MXFP4_QUANT_BLOCK_SIZE // 2, 2])
    evens, odds = tl.split(e2m1_value)
    out_tensor = evens | (odds << 4)

    out_offs_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE) 
    out_offs_n = pid_n * MXFP4_QUANT_BLOCK_SIZE // 2  + tl.arange(0, MXFP4_QUANT_BLOCK_SIZE // 2) 
    out_offs = out_offs_m[:, None] * stride_x_fp4_m + out_offs_n[None, :] * stride_x_fp4_n
    out_mask = (out_offs_m < M)[:, None] & (out_offs_n < (N // 2))[None, :]
    tl.store(x_fp4_ptr + out_offs, out_tensor, mask=out_mask)

    bs_offs_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE) 
    bs_offs_n = pid_n
    bs_offs = bs_offs_m[:, None] * stride_bs_m + bs_offs_n[None, :] * stride_bs_n
    bs_mask = (bs_offs_m < M)[:, None] & (bs_offs_n < N)[None, :]
    tl.store(bs_ptr + bs_offs, bs_e8m0, mask=bs_mask)

def dynamic_mxfp4_quant(x: torch.Tensor, scaling_mode: str = "even") -> tuple[torch.Tensor, torch.Tensor]:
    """
    Quantize a tensor to MX FP4 format.

    Args:
        x: The input tensor, typically fp16 or bf16.
        scaling_mode: The method to calculate MX block scaling.
            - "even" (default): `even_round` in `quark.torch.quantization.utils`.
            - etc.
    Returns:
        A tuple of (x_fp4, blockscale_e8m0).
    """
    #Assume x is 2D-Tensor for now
    M, N = x.shape

    assert (N // 2) % 2 == 0

    #This is fixed by spec for MXFP4. Do not tune this.
    #For performance, perhaps, we should look at passing multiple of 32 column blocks 
    #that a triton program can process
    MXFP4_QUANT_BLOCK_SIZE = 32 

    x_fp4 = torch.empty((M, N // 2), dtype=torch.uint8, device=x.device)
    blockscale_e8m0 = torch.empty((M, (N + MXFP4_QUANT_BLOCK_SIZE - 1) // MXFP4_QUANT_BLOCK_SIZE), dtype=torch.uint8, device=x.device)


    BLOCK_SIZE = 128
    grid = (triton.cdiv(M, BLOCK_SIZE), triton.cdiv(N, MXFP4_QUANT_BLOCK_SIZE))
    _dynamic_mxfp4_quant_kernel[grid](x,
                                x_fp4,
                                blockscale_e8m0,
                                *x.stride(), *x_fp4.stride(), *blockscale_e8m0.stride(),
                                M=M,
                                N=N,
                                BLOCK_SIZE=BLOCK_SIZE,
                                MXFP4_QUANT_BLOCK_SIZE=MXFP4_QUANT_BLOCK_SIZE,
                                SCALING_MODE=0
                            )

    
    return (x_fp4, blockscale_e8m0)


